{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Technical setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/lightonai/lightonmuse/releases/download/v0.2.1/lightonmuse-0.2.1-py3-none-any.whl\n",
    "!pip install lightonmuse-0.2.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from lightonmuse import Create, Select\n",
    "import os\n",
    "\n",
    "# IMPORTANT: Add your Muse API key below. If you don't have one, contact muse@lighton.ai.\n",
    "#os.environ['MUSE_API_KEY'] = \"YOUR-MUSE-API-KEY\"\n",
    "\n",
    "creator_en = Create(\"lyra-en\")\n",
    "creator_fr = Create(\"orion-fr-v2\")\n",
    "creator_fr_summary = Create(\"orion-fr\")\n",
    "\n",
    "selector_en = Select(\"lyra-en\")\n",
    "selector_fr = Select(\"orion-fr-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßë‚Äçüíª `lyra-en`, our large English model\n",
    "\n",
    "`lyra-fr` is **our most powerful and capable English model**. As a `lyra` model, it is able to tackle specialized text generation\n",
    " tasks, as well as complex zero/few-shot tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí≠ Creative Writing -- Freeform generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extract from Seveneves, Neal Stephenson.\n",
    "prompt = \"The Moon blew up without warning for no apparent reason. It was waxing, only one day short of full. The time was 05:03:12 UTC. Later it would be designated A+0.0.0, or simply Zero.\\nAn amateur astronomer in Utah was the first person on Earth to realize that something unusual was happening. Moments earlier, he had noticed a blur flourishing in the vicinity of the Reiner Gamma formation, near the moon‚Äôs equator. He assumed it was a dust cloud thrown up by a meteor strike.\"\n",
    "print(\"Prompt | \" + prompt)\n",
    "\n",
    "out = creator_en(prompt, temperature=1.0, n_tokens=65, seed=0)\n",
    "print(\"\\n\\nü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÉ Copywriting -- Guided generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"An Instagram ad for Atmosphere resort, a luxury hotel on Apo island in the Philippines with world-class diving.\\n\\nInstagram Ad:\" \n",
    "print(\"Prompt | \" + prompt)\n",
    "\n",
    "out = creator_en(prompt, temperature=0.8, n_tokens=98, seed=0, \n",
    "                  word_biases={\"luxury\": 5, \"scuba\": 5}, frequency_penalty=0.5) # We use biases to guide generation.\n",
    "print(\"\\n\\nü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ú® Marketing -- Automated review answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Answer the following client reviews.\\n\\n###\\n\\nReview: Really innovative and cool vibe. Cocktails are unusual sounding, but great tasting. We loved the service too, not at all pretentious, friendly and attentive. We‚Äôll be back!\\nAnswer: Thank you for your positive review! We hope to see you again soon :).\\n\\n###\\n\\nReview: Great atmosphere. Poor service and drinks.  Waited 45 minutes for our drinks.  The mojito was like Sprite with mint garnish.  They didn‚Äôt seem to care.  For a bar based on cocktails, super poor.  Expensive and overrated.  Go elsewhere\\nAnswer: We are sorry we didn't meet your expectations, and that you had to wait for so long. We hope you give us another chance soon.\\n\\n###\\n\\nReview: I like this bar. The deco, the staff and the little bit of a mystery in finding it all adds up to a positive experience.\\nAnswer:\" \n",
    "print(\"Prompt | \" + prompt, end=\" \")\n",
    "\n",
    "out = creator_en(prompt, temperature=0.8, n_tokens=25, seed=0, \n",
    "                  stop_words=[\"\\n\", \".\", \"\\n\\n\"]) # We are using stop words to generate only one response.\n",
    "                  # Using a lower temperature would create shorter (e.g. 0.6), more direct answers. \n",
    "print(\"ü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßë‚Äçüè´ Open Q&A -- Statistics & machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt =\"Question: What is a statistical model?\\nAnswer: A statistical model is a mathematical model that describes the probability of an event.\\nQuestion: What is an artificial neural network?\\nAnswer:\"\n",
    "print(\"Prompt | \" + prompt, end=\" \")\n",
    "\n",
    "out = creator_en(prompt, temperature=1.0, seed=0, \n",
    "                  stop_words=[\".\", \"!\", \"...\", \"\\n\\n\"]) # We are using stop words to generate only one response.\n",
    "print(\"ü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Chatbot -- Python helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Eve is an helpful chatbot knowledgeable about Python.\\n\\nYou: How do I sort an array?\\nEve: You can use the sort() method.\\nYou: How do I remove the last element of an array?\\nEve: \"\n",
    "print(\"Prompt | \" + prompt, end=\" \")\n",
    "\n",
    "out = creator_en(prompt, temperature=0.9, seed=0, \n",
    "                  stop_words=[\".\", \"!\", \"...\", \"\\n\\n\"]) # We are using stop words to generate only one response.\n",
    "print(\"ü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëç Classification -- Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"This is a review sentiment classifier.\\n\\nReview: 'The new Dune movie is great!'\\nSentiment: Positive.\\n###\\nReview: 'Suicide Squad was an awful movie, that's 2 hours of my life I won't get back...'\\nSentiment: Negative.\\n###\\nReview: 'Wooh, just came out of Interstellar and it was amazing.'\\nSentiment: Positive.\\n###\\nReview: 'TBH Squid Game wasn't worth my time.'\\nSentiment:\"\n",
    "print(\"Prompt | \" + prompt, end=\" \")\n",
    "\n",
    "out = creator_en(prompt, temperature=0.1, seed=0, # Very low temperature as there is no need for creativity.\n",
    "                  stop_words=[\".\", \"!\", \"...\", \"\\n\\n\"]) # We are using stop words to generate only one response.\n",
    "print(\"ü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè¢ Open-ended classification -- Company categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"The following is a list of companies and the categories they fall into\\n\\nMeta: Social media, Virtual Reality\\nLinkedIn: Social media, Careers\\nDeliveroo: Logistics, Food, Marketplace\\nUber: Transportation, Marketplace\\nUnilever: Conglomerate, Consumer Goods\\nMcDonalds: Fast Food, Restaurants\\nGoogle:\"\n",
    "print(\"Prompt | \" + prompt, end=\" \")\n",
    "\n",
    "out = creator_en(prompt, temperature=0.3, seed=0, # Low temperature as we don't need much creativity.\n",
    "                  stop_words=[\".\", \"!\", \"...\", \"\\n\\n\", \"\\n\"]) # We are using stop words to generate only one response.\n",
    "print(\"ü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Customer support -- Classification with log-probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Hey! I let my subscription lapse last month and can't connect anymore to the platform... Can you help?\"\n",
    "classes = [\"This user is asking for technical support.\", \"This user wants to cancel his subscription.\", \"This user is giving feedback.\"]\n",
    "print(\"Prompt | \" + prompt)\n",
    "print(\"Candidates | \" + str(classes))\n",
    "\n",
    "out = selector_en(prompt, classes) # When using log-probabilities, we don't need to think about sampling.\n",
    "print(\"ü§ñ | \" + out[0][0]['best'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üôã Q&A -- Using log-probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Who was the president of France in 2015?\"\n",
    "classes = [\"Emmanuel Macron.\", \"Fran√ßois Hollande.\", \"Barack Obama.\"]\n",
    "print(\"Prompt | \" + prompt)\n",
    "print(\"Candidates | \" + str(classes))\n",
    "\n",
    "out = selector_en(prompt, classes) # When using log-probabilities, we don't need to think about sampling.\n",
    "print(\"ü§ñ | \" + out[0][0]['best'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Text transformation -- Autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Correct sentences into proper English.\\n\\nIncorrect: Can we use there house?\\nCorrect:\"\n",
    "print(\"Prompt | \" + prompt, end=\" \")\n",
    "\n",
    "out = creator_en(prompt, temperature=0.2, seed=0, # Low temperature to remain factual and close to original text.\n",
    "                     stop_words=[\"\\n\"], n_tokens=25) # We are using stop words to generate only one answer.\n",
    "print(\"ü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Retrieval -- Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Text: Double Asteroid Redirection Test (DART) is a NASA space mission aimed at testing a method of planetary defense against near-Earth objects (NEO). It will deliberately crash a space probe into the double asteroid Didymos' moon, Dimorphos, to test whether the kinetic energy of a spacecraft impact could successfully deflect an asteroid on a collision course with Earth. DART is a joint project between NASA and the Johns Hopkins Applied Physics Laboratory (APL), administered by NASA's Planetary Defense Coordination Office, with several NASA laboratories and offices providing technical support. International partners, such as the space agencies of Europe, Italy, and Japan, are contributing to related or subsequent projects. In August 2018, NASA approved the project to start the final design and assembly phase. DART was launched on 24 November 2021, at 06:21:02 UTC, with collision slated for 26 September 2022.\\n\\nKeywords:\"\n",
    "print(\"Prompt | \" + prompt, end=\" \")\n",
    "\n",
    "out = creator_en(prompt, temperature=0.7, seed=0,\n",
    "                     stop_words=[\"\\n\"], presence_penalty=1., n_tokens=15) # We use a presence_penalty to encourage different keywords\n",
    "print(\"ü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Text transformation -- Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more if the star was especially metal-rich.[1] Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical white holes, quark stars, and strange stars.[2] Neutron stars have a radius on the order of 10 kilometres (6.2 mi) and a mass of about 1.4 solar masses.[3] They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core past white dwarf star density to that of atomic nuclei.\\n\\ntl;dr:\"\n",
    "print(\"Prompt | \" + prompt, end=\" \")\n",
    "\n",
    "out = creator_en(prompt, p=0.9, temperature=0.8, seed=0, n_completions=3, n_best=1, # Best of helps create higher quality texts.\n",
    "                     stop_words=[\"!\", \"...\", \"\\n\"], n_tokens=100)\n",
    "print(\"ü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üá´üá∑ `orion-fr-v2`, our new-generation French model\n",
    "\n",
    "`orion-fr-v2` is our first **next-generation French model**, trained on curated data from 2021. As an `orion` model, it offers\n",
    "a great trade-off between power and cost, and is ideally suited to text generation, and guided classification with\n",
    "üîò Select."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí≠ √âcriture libre -- Cr√©ation litt√©raire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extrait de √Ä l'ombre des jeunes filles en fleurs, Marcel Proust.\n",
    "prompt = \"C'√©tait un temps que je connaissais ; j'eus la sensation et le pressentiment que le jour de l'an n'√©tait pas un jour diff√©rent des autres, qu'il n'√©tait pas le premier d'un monde nouveau o√π j'aurais pu, avec une chance encore intacte, refaire la connaissance de Gilberte comme au temps de la Cr√©ation, comme s'il n'existait pas encore de pass√©, comme si eussent √©t√© an√©anties, avec les indices qu'on aurait pu en tirer pour l'avenir, les d√©ceptions qu'elle m'avait parfois caus√©es : un nouveau monde o√π rien ne subsist√¢t de l'ancien‚Ä¶\"\n",
    "print(\"Prompt | \" + prompt)\n",
    "\n",
    "out = creator_fr(prompt, temperature=1., n_tokens=65, seed=0)\n",
    "print(\"\\n\\nü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßë‚Äçüè´ Question/r√©ponse conversationnel -- Statistique et apprentissage automatique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt =\"Question: Qu'est ce qu'un mod√®le statistique?\\nR√©ponse: Un mod√®le statistique est un mod√®le qui d√©crit la probabilit√© d'un √©v√©nement.\\nQuestion: Qu'est ce qu'un r√©seau de neurones?\\nR√©ponse:\"\n",
    "print(\"Prompt | \" + prompt, end=\" \")\n",
    "\n",
    "out = creator_fr(prompt, temperature=1., seed=0, n_tokens=50,\n",
    "                  stop_words=[\"\\n\"]) # We are using stop words to generate only one response.\n",
    "print(\"ü§ñ | \" + out[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üôã Question/r√©ponse structur√© -- Culture g√©n√©rale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Qui √©tait le pr√©sident de la France en 1955 ?\"\n",
    "classes = [\"Vincent Auriol.\", \"Ren√© Coty.\", \"Emmanuel Macron.\", \"Charles de Gaulle.\"]\n",
    "print(\"Prompt | \" + prompt)\n",
    "print(\"Candidates | \" + str(classes))\n",
    "\n",
    "out = selector_fr(prompt, classes) # When using log-probabilities, we don't need to think about sampling.\n",
    "print(\"ü§ñ | \" + out[0][0]['best'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Classification structur√©e -- Support client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Bonjour ! J'ai eu un probl√®me de paiement de mon abonnement le mois dernier et je n'arrive plus √† acc√©der √† mon compte... Pouvez-vous m'aider ?\"\n",
    "classes = [\"Cet utilisateur √† besoin d'une aide technique.\", \"Cet utilisateur veut annuler son abonnement.\", \"Cet utilisateur donne son avis.\"]\n",
    "print(\"Prompt | \" + prompt)\n",
    "print(\"Candidates | \" + str(classes))\n",
    "\n",
    "out = selector_fr(prompt, classes) # When using log-probabilities, we don't need to think about sampling.\n",
    "print(\"ü§ñ | \" + out[0][0]['best'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§π `orion-fr`, summarization skill\n",
    "\n",
    "With **ü§π [Skills](https://muse.lighton.ai/docs/api/skills)**, you can **specialize models** in the API to perform specific tasks.\n",
    "Here we demonstrate the *summarization* skill, which generates a summary of the provided prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Un an √† peine apr√®s la normalisation de leurs relations, le Maroc et Isra√´l ont fait un pas spectaculaire l‚Äôun vers l‚Äôautre, mercredi 24 novembre. Les deux pays ont conclu un accord-cadre de coop√©ration s√©curitaire ¬´ sans pr√©c√©dent ¬ª, lors d‚Äôune visite historique √† Rabat du ministre isra√©lien de la d√©fense, Benny Gantz, en pleine tension entre le royaume ch√©rifien et l‚ÄôAlg√©rie.\n",
    "M. Gantz, un ancien chef de l‚Äôarm√©e isra√©lienne, a √©t√© re√ßu en d√©but de matin√©e par le ministre d√©l√©gu√© charg√© de l‚Äôadministration de la d√©fense nationale marocaine, Abdellatif Loudiyi. Ils ont sign√© un protocole d‚Äôaccord qui lance formellement la coop√©ration s√©curitaire ¬´ sous tous ses aspects ¬ª entre les deux pays, face aux ¬´ menaces et d√©fis dans la r√©gion ¬ª, selon la partie isra√©lienne. ¬´ Il s‚Äôagit d‚Äôune chose tr√®s importante qui nous permettra aussi d‚Äô√©changer nos opinions, de lancer des projets conjoints et favorisera les exportations isra√©liennes jusqu‚Äôici ¬ª, a soulign√© M. Gantz.\n",
    "Avant son d√©part de Tel-Aviv, mardi soir, il avait √©voqu√© ¬´ un voyage important au Maroc qui a une touche historique, car il s‚Äôagit de la premi√®re visite formelle d‚Äôun ministre de la d√©fense [isra√©lien] dans ce pays ¬ª. Au cours de ce d√©placement de quarante-huit heures, M. Gantz doit aussi s‚Äôentretenir, mercredi, avec le ministre marocain des affaires √©trang√®res, Nasser Bourita.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texte : \" + prompt)\n",
    "outputs = creator_fr_summary(prompt, n_tokens=50, skill=\"summarisation\", seed=3) # We use skills to dynamically adapt the model to specific tasks.\n",
    "\n",
    "print(\"R√©sum√© : ü§ñ \" + outputs[0][0]['completions'][0]['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Demo LightOn Muse",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
